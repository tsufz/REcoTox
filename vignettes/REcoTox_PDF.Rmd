---
title: "REcoTox - a workflow to process US EPA ECOTOX Knowledgebase ASCII files (v`r desc::desc_get_version()`)"
shorttitle: "REcoTox (version `r desc::desc_get_version()`)"
author: |
        | Tobias Schulze
        | Helmholtz Centre for Environmental Research - UFZ, Leipzig, Germany
        | tobias.schulze@ufz.de
date: 2023-09-30
output:
  bookdown::pdf_document2:
    toc: true
    toc_depth: 2
    number_sections: true
    latex_engine: "pdflatex"
    global_numbering: true
vignette: >
  %\VignetteIndexEntry{REcoTox - a workflow to process US EPA ECOTOX Knowledgebase ASCII files (PDF version)}
  %\VignetteKeywords{E}
  %\VignettePackage{REcoTox}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteDepends{BiocStyle, desc}
bibliography: references.bib
csl: biomed-central.csl
editor_options: 
  markdown: 
    wrap: 72
---

```{r biocstyle, echo = FALSE, results = "asis"}
BiocStyle::markdown()
```

```{r init, message = FALSE, echo = FALSE, results = "hide" }
## Silently loading all packages
library(BiocStyle)
library(desc)
library(kableExtra)
library(tidyverse)
```

\newpage

# Background
The search and extraction of experimental ecotoxicological information is often
a tedious work. A good and comprehensive data source is the
[US EPA ECOTOX Knowledgebase](https://cfpub.epa.gov/ecotox/ "US EPA ECOTOX Knowledgebase").
It contains more than 1 million data points for almost 13,000 chemicals
and 14,000 single species. However, for a high-throughput hazard assessment,
it is not possible to extract all relevant data of the online database.
The purpose of REcoTox is to extract the relevant information and to aggregate
the data based on the user criteria out of the entire database
[ASCII files](https://gaftp.epa.gov/ecotox/ecotox_ascii_03_10_2022.zip "ECOTOX Knowledgebase ASCII files").

# Introduction

[REcoTox](https://github.com/tsufz/REcoTox) is a semi-automated, interactive
workflow to process [US EPA ECOTOX Knowledgebase](https://cfpub.epa.gov/ecotox/ "US EPA ECOTOX Knowledgebase")
entire database [ASCII files](https://gaftp.epa.gov/ecotox/ecotox_ascii_03_10_2022.zip "ECOTOX Knowledgebase ASCII files")
to extract and process ecotoxicological data relevant (but not restricted) to
the ecotoxicity groups algae, crustaceans, and fish in the aquatic domain.
The focus is aquatic ecotoxicity and the unit of the retrieved data is `mg/L`.

\newpage

# Input files and folders

`REcoTox` requires an unzipped `US EPA Knowlegdebase` database in `ASCII` format (Zitat).
The database is preferable expanded in an own database folder to be defined during the processing,
The database consists of relatively referenced text files. The separator of the data is the pipeline `|` symbol.

In the first session of `REcoTox`, a file `chemical_properties.csv` is created in the database folder.
This files contains chemical identifiers and chemical properties required for the processing of the
chemical data in the knowlegdebase and to tag the results.

The chemical property file is dynamically updated and requires also some manual curation. It will grow as soon
new chemicals are added to the knowledgebase.

The `project_folder` contains the `R` script for processing as well as the intermediate and final processing files.
The naming of the folder is arbitrary, but do not use spaces, but underscores (`_`) or hyphens (`-`) for separating
parts.

To run the queries, a predefined processing script is available on `GitHub` ([`Query_EcoTox_DB.R`](https://github.com/tsufz/REcoTox/blob/main/inst/extdata/Query_Ecotox_DB.R)) or in the local `REcoTox` package folder.


\newpage

# Using REcoTox

The following tutorial explains the different steps of `REcoTox` in a
comprehensive demonstration. `REcoTox` includes different interactive
steps, which require the evaluation of comma separated text files
(`*.csv`) in an external spreadsheet application (preferable LibreOffice
[@LibreOffice]).

## Load the REcoTox package

```{r load REcoTox package, eval = TRUE, echo = TRUE, message = FALSE, warning = FALSE}
# Load the REcoTox package
library(REcotox)
```

## Documentation for MZquant

A detailed description of all functions of `REcoTox` functions is available in the
`R Documentation`.

```{r R Documentation, echo = TRUE, eval = FALSE}
# Documentation of REcoTox
help(package = "REcoTox")
```

## Preparation of the working environment (for beginners)

The processing in `REcoTox` is interactivally controlled by a processing
script `Query_EcoTox_DB.R`.

If you run `REcoTox` for the first time, a tutorial project is available
to demonstrate all important steps of `REcoTox` processing. The following
script is preparing an example folder in your home directory and copies
all necessary files in the folder.

```{r initialize folders, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# Path of the project folder
project_folder <- "REcoTox_demo"
database_folder <- system.file("extdata", package="REcoTox")

# The project folder is created in the home directory
project_folder <- normalizePath(ifelse(.Platform$OS.type == "unix",
    paste0("~/", project_folder),
    paste0(
        Sys.getenv("HOMEPATH"),
        "\\",
        project_folder
    )
))

# An existing folder is deleted
if (dir.exists(project_folder)) {
    unlink(project_folder, recursive = TRUE)
}
```

This command initializes the project folder and the database folder. It copies also the processing script to the project folder.

```{r initialize project, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
project <- create_project(database_path,
                          project_path,
                          initalise_database_project = TRUE, # create the basic project from current ASCII files in DB folder
                          initalise_project = TRUE, # initializes the project folder
                          load_default = FALSE) # loads the default project in the project folder in the memoryfault_example = TRUE

file.copy(
                    from = system.file(
                        "extdata",
                        "Query_EcoTox_DB.R",
                        package = "REcoTox"
                    ),
                    to = normalizePath(
                        path = file.path(
                            project_folder,
                            "Query_EcoTox_DB.R"
                        ),
                        winslash = "\\",
                        mustWork = FALSE
                    ),
                    overwrite = TRUE
                )

```

The `project_folder` contains the following files:

```{r list project folder}
# List files and directories in project_folder
list.files(project_folder, recursive = TRUE, include.dirs = TRUE)
```

The `database_folder` contains the following files and folders:
`chemical_properties.csv` is the file containing the curated chemical properties,
`results.txt` contains the testing results collected in the knowledgebase, and
`test.txt` contains the the metadate of the tests.

The folder `validation` contains the files `chemicals.txt` with chemical information,
the file `references.txt` contains the references and `species.txt` the species.

```{r list project folder}
# List files and directories in project_folder
list.files(database_folder, recursive = TRUE, include.dirs = TRUE)
```

It contains only the `Query_EcoTox_DB.R` file.

## Review of the input data

To review the input data, let us look in the data:

```{r view chemical_properties, echo = TRUE, eval = TRUE, message = TRUE}
# Review of the chemical properties
chemical_properties <- readr::read_csv(file = normalizePath(path = file.path(
    database_folder,
    "chemical_properties.csv"
), ), show_col_types = FALSE)

kable(
    samples %>%
        head(5),
    format = "latex", digits = 2
)
```

```{r view results, echo = TRUE, eval = TRUE, message = TRUE}
# Review of the substance_table
substances <-
    readr::read_delim(
        file = normalizePath(
            path = file.path(
                project_folder,
                "results.txt"
            ),
        ),
        show_col_types = FALSE,
        delim = "|"
        
    )

kable(
    substances %>%
        head(5),
    format = "latex", digits = 2
)
```

```{r view tests, echo = TRUE, eval = TRUE, message = TRUE}
# Review of the substance_table
substances <-
    readr::read_delim(
        file = normalizePath(
            path = file.path(
                project_folder,
                "results.txt"
            ),
        ),
        show_col_types = FALSE,
        delim = "|"
        
    )

kable(
    substances %>%
        head(5),
    format = "latex", digits = 2
)
```


## Preparation of the working environment (for experienced users)

Experienced users can re-use existing `MZquant_processing_script.R` and
`MZquant_settings.yaml` files.^[Note that the workflow and the settings
file can change in a new package version and thus a review of the change
log (NEWS) is recommended.]

The steps to setup a custom project are:

1.  Create a project folder.^[It is recommended to use the same name as the
    analytical batch.]

2.  Copy the custom `MZquant_substances.csv` and the
    `MZquant_samples.csv` in the folder.

3.  Re-use existing `MZquant_processing_script.R` and/or
    `MZquant_settings.yaml` files.

4.  Or, create new files with the following command.

```{r initialize new processing files, echo = TRUE, message = FALSE, warning = FALSE, eval = FALSE}

# Initialization and setup of the project folder
project_folder <- "your_folder"
settings_file <- "MZquant_settings.yaml"

# default_settings = TRUE loads the default settings file
# default_processing_script = TRUE loads the default processing script
initialise_project(project_folder, settings_file,
    default_processing_script = TRUE,
    default_settings = TRUE,
    default_example = FALSE
)
```

The command will initialize the project folder and copy the default
`MZquant_settings.yaml` and the default `MZquant_processing_script.R` in
your project folder. If you copied existing files in step 3 with similar
names, they will be overwritten by step 4.

## MZquant processing and settings

Open the `MZqant_processing_script.R` and the settings file
`MZquant_settings.yaml` for review and further processing.

```{r load processing and settings file, echo = TRUE, eval = FALSE}

# Open the processing script
file.edit(file.path(project_folder, "MZquant_processing_script.R"))

# Open the settings file
file.edit(file.path(project_folder, settings_file))
```

### The processing script

The `MZquant_processing_script.R` contains all necessary steps of the
MZquant workflow described in this vignette.

The data of the `MZquant` project is stored in a hidden environment
`.MZquant.env`.

To review the hidden environment, you may call.

```{r call enviroment, eval = FALSE, echo = TRUE}
View(.MZquant.env)
```

The `.MZquant.env` is stored after each processing step to ensure easy
redo of single processing steps without need to repeat the whole
analysis.

### The settings file

The settings file `MZquant_settings.yaml` contains all necessary and
customization settings. Scroll thought the document and make all
required edits, for example:

-   How are the blanks tagged (e.g., `Blank`)?
-   How are the calibration columns tagged?
-   How are the quality control columns tagged?
-   Should a blank correction be performed?
-   Which type of blank correction is performed?

Finally, save the file and go back to the processing script
`MZquant_processing_script.R`.

The settings file is read in the current `.MZquant.env` environment by
running:

```{r read in settings, eval = FALSE, echo = TRUE}
# Call the read_settings function to read the filed settings
# into the current `.MZquant.env`
read_settings(settings_file, project_folder)
```

The `read_settings()` function is especially helpful if settings are
changed during analysis or old projects are reprocessed with a newer
version of `MZquant`. In the latter case, new features or bug fixes can
be easily applied without need of the tedious reprocessing of the entire
data.

## Feature list workflow

The `feature_list_workflow` reads the `MZquant_samples.csv` and the
`MZquant_substances.csv`. It prepares the feature tables for the
analysis in MZquant.

### Feature list workflow step 1

Step 1: Read `MZmine 3` exported feature list `MZquant_samples.csv` and
the substance file `MZquant_substances.csv`.

```{r}
#| feature list workflow step 1, echo = TRUE,
#| message = FALSE, warning = FALSE, eval = TRUE

# Feature list workflow step 1
feature_list_workflow(step = 1)
```

-   Read the `MZmine 3` exported feature list and the `substance_table`.

-   Merge the tables.

-   Assign standard names (`compound_db_identity:stdname`) to the
    annotations in the `feature list` and the `substance_table`.

-   Assign the response method set in
    `MZquant_settings.yaml:data:response`.

-   Select mandatory and custom fields set in
    `MZquant_settings.yaml:data:meta_mzmine`.

-   Export the new table to `MZquant_samples_refined.csv` in the
    `project_folder`.

The `feature_liste_workflow` step 1 applies a `regular expression` to
the `compound_identity` in the `substance_table`, respectively, to the
column `compound_db_identity:compound_db_identity` in the
`MZquant_samples.csv` to replace any punctuation in the names to ensure
unequivocal standardized names called `stdname`. The `stdname` is one of
the `keys` used in `MZquant`.

The regular expression is:

$$stdname = stringi::stri\_replace\_all\_regex(name, "[[:punct:] \backslash \backslash s]+", "\_")$$

For example, `3,3'-Dichlorobenzidine` is expressed as
`3_3_Dichlorobenzidine`, which is unequivocally processing.

In the `samples_refined.csv`, the `stdname` is stored in
`compound_db_identity:compound_db_identity:stdname.`

Review this table. For experienced users: Take the opportunity to edit
the table to your purposes, for example, add missing QC data by copying
of calibration data.

*Optional: Load the environment after running
`quantification_workflow(step = 1)`*

The working environment is stored after each processing step in the root
of the project folder. To reload the environment, run the `load`
command. This is for example helpfully, if a processing required to
break for a while or a `bug` in a later step was tracked down.

```{r load feature_list_workflow_1.RData, echo = TRUE, eval = FALSE, message = FALSE}
# Loads the environment after running `feature_list_workflow(step = 1)
load(
    file = file.path(project_folder, "MZquant_feature_list_workflow_1.RData"),
    envir = .GlobalEnv
)
```

### Feature list workflow step 2

Step 2: Read and preprocess the refined feature list
`MZquant_samples_refined.csv`.

```{r feature list workflow step 2, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# Feature list workflow step 2
feature_list_workflow(step = 2)
```

-   Read the MZmine export.csv file and the `substance_table`.

-   Prepare the internal feature list structure.

-   Match the annotations in the feature list with the
    `substance_table`.

-   Filter missing substances.

The columns in `MZquant_settings.yaml:data:meta_mzmine` and
`MZquant_settings.yaml:data:meta_substances` are appended. The
`feature_list_workflow` step 2 stores three files in the
`results/feature_lists` folder.

```{r list feature_list folder, echo = TRUE}
# List files and directories in the feature list folder
list.files(file.path(project_folder, "results", "feature_lists"),
    include.dirs = TRUE
)
```

The `target_table_for_review.csv` is the relevant spreadsheet for all
edits in the annotated feature list. Open the list in your favorite
spreadsheet application. To preserve the edits for later review, the
file could be saved in the native spreadsheet format (e.g. `ods` or
`xlsx`).

The only columns to be edited (do not edit any other column or delete
any row or entry, this will have side effects and you need to repeat
this step):

-   `remove_annotation` -\> set to `1` for removing annotations

-   `newstdname` -\> add a new `stdname` (from
    `substance_table_for_review.csv`), if necessary (e.g., in case of
    *duplicate masses*)

-   `new_class` -\> class of the annotation (for example of an internal
    standard), `c(Target, Suspect, Internal Standard)`

See the green highlighted columns in [Figure 3](#fig3).

#### Tips for the data review

**Hide columns, use auto filters and freeze rows/columns ([Figure
3](#fig3))**

-   Hide all columns you do not need for review (e.g., `stdname`,
    `compound_id`, `mzquant_id`).

-   Mark the first row and add the auto filter (data \| AutoFilter).

-   Use the Freeze Rows and Columns functions to freeze the header and
    the column with the `StdName`.

-   Columns could be re-arranged, but never delete columns or remove
    rows!

**Hide columns, use auto filters and freeze rows/columns**

-   Use color scales to mark the the blank data the sample data ([Figure
    4](#fig4)).

-   Use the scientific notation for theses columns for straightforward
    comparison.

-   Add `duplicate` conditional formatting to column `stdname` to
    highlight duplicate names ([Figure 5](#fig5))

<a id="fig3"></a>

```{r}
#| fig3, fig.cap = "Hide and freeze columns.",
#| fig.align = "center", echo = FALSE, out.width = "80%",
#| eval = TRUE
knitr::include_graphics("./figures/Figure_3.png")
```

<a id="fig4"></a>

```{r}
#| fig4, fig.cap = "Highlight the data with color scales.",
#| fig.align = "center", echo = FALSE, out.width = "100%",
#| eval = TRUE
knitr::include_graphics("./figures/Figure_4.png")
```

<a id="fig5"></a>

```{r}
#| fig5,
#| fig.cap = "Use conditional formatting to annotate duplicates in stdname and remove\\_annotation.",
#| fig.align = "center",
#| echo = FALSE, out.width = "75%", eval = TRUE
knitr::include_graphics("./figures/Figure_5.png")
```

**Data review**

The selection of the correct annotation is not always straightforward.
`MZquant` delivers different qualifiers to support the decision ([Figure
6](#fig6)).

-   Decision criteria, review the following columns:

    -   expected `mz` from `MZquant_substances.csv`

    -   expected `rt` from `MZquant_substances.csv`

    -   detected `row_mz` in MZmine 3

    -   detected `row_rt` in MZmine 3

    -   `annotation_score` estimated in MZmine 3

    -   `row_mz_delta_ppm` estimated in MZmine 3

    -   `deltart` as the difference between expected and detected
        retention time

    -   `deltamz` as the difference between expected and detected mz

    -   `mz_ppm_error` in ppm estimated in MZquant

    -   \`quantification_monotonicity\`\` to show monotonicity of the
        quantification levels

    -   `counts` in blanks, calibrations, samples, and quality controls

    -   `blank_features_threshold`^[The `blank_features_threshold` is
        the value which is calculated by the blank correction and is
        the level of blank noise. All features with lower intensities
        than the threshold will be removed in the blank_workflow.]

    -   maximum value in the samples

<a id="fig6"></a>

```{r}
#| fig6, fig.cap = "Criteria for the removal of duplicate annotations.",
#| fig.align = "center", echo = FALSE, out.width = "90%",
#| eval = TRUE
knitr::include_graphics("./figures/Figure_6.png")
```

-   Lookup for compounds with low findings in the samples, e.g.:

    -   select for example those records with low sample annotations

    -   lookup for the better annotation score

    -   check if the compound occurs only in the first consecutive
        sample following the highest calibration level

    -   check for their intensity, if they occur only at the noise level
        (1e4. 1e3, compare to blanks), they could be carry over from the
        calibration sample

    -   check if they only occur in QC samples and so on

    -   remove the annotation if the compound is carry over, not
        occurring in samples etc. by setting the value in
        `remove_annotation` to `1`

    -   **Note**: because of the blank correction, some compound without
        values in the samples will occur in the trimming table

-   Lookup for duplicates

    -   Duplicates should be removed from the table; they occur from
        broad peaks, isobaric compounds, retention time shifts, etc.

        -   Check the comment field and lookup for existing entries, for
            example ([Figure 5](#fig5)):

            -   duplicate mass -\> isobaric compounds

            -   check the `missed_substances_for_review.csv` and search
                for the `unit mass`

            -   compare the retention time

            -   copy the `StdName` from the missed table to the field
                `newStdName` to rename the tagging of the compound

In addition a screening of the shapes of the peaks in `MZmine 3` is also
helpful, does keep `MZmine 3` always open until the end of your analysis
([Figure 7](#fig7)). However, the decision is also not always
straightforward, if shapes fit well in all cases ([Figure 8](#fig8)).

It is not always an easy decision, the `deltart` / `deltamz` and the
`mz_ppm_error` might be good for some records, but the ratios
`QC`/`calibration` does not fit well → in cases of concern ask your
supervisors or discuss with colleagues.

Mark all duplicates to be removed by setting the value in
`remove_annotation` to `1` ([Figure 5](#fig5)) or re-annotate them by
setting a new name in `new_StdName`.Lookup for other curious compounds
and remove them if you like.

**Note: If you change to scientific notation, change back to `General`
(`LibreOffice`) or `Standard` (`Excel`) formats to avoid loss of
precision!** In recent versions of `Excel`, it is possible to save data
to `CSV UTF-8 (Comma delimited) (*.csv)`. This is recommended.

<a id="fig7"></a>

```{r}
#| fig7,
#| fig.cap = "Peak shapes of duplicate annotations in MZmine 3. An example with straightforward decision.",
#| fig.align = "center", echo = FALSE, out.width = "100%",
#| eval = TRUE
knitr::include_graphics("./figures/Figure_7.png")
```

<a id="fig8"></a>

```{r}
#| fig8,
#| fig.cap = "Peak shapes of duplicate annotations in MZmine 3. An example, that needs more decision criteria.",
#| fig.align = "center", echo = FALSE, out.width = "90%",
#| eval = TRUE
knitr::include_graphics("./figures/Figure_8.png")
```

*Optional: Load the environment after running
`quantification_workflow(step = 2)`*

```{r load feature_list_workflow_2.RData, echo = TRUE, eval = FALSE, message = FALSE}
# Loads the environment after running `feature_list_workflow(step = 2)
load(
    file = file.path(project_folder, "MZquant_feature_list_workflow_2.RData"),
    envir = .GlobalEnv
)
```

To override the revision step in the table, an edited table can be
loaded in the environment.

```{r read the demonstration target table, echo = TRUE, eval = TRUE, message = FALSE}
# Load the edited  `target_table_for_review.csv`
copy_demo_target_table(project_folder = project_folder)
```

### Feature list workflow step 3

After successful edit of `target_table_for_review.csv`, the table is
loaded and processed in `feature_list_workflow` step 3. This step
updates the `feature list` and prepares the final `feature list` for
further processing.

```{r feature list workflow step 3, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# feature list workflow step 3
feature_list_workflow(step = 3)
```

In the case, duplicates are still not annotated, `MZquant` will warn
([Figure 9](#fig9)). In this case, review the `target_table_for_review.csv`
and repeat `feature_list_workflow(step = 2)`.^[This feature can be also used
to quickly check for duplicates.]

<a id="fig9"></a>

```{r}
#| fig9, fig.cap = "Remaining duplicate mass compounds.",
#| fig.align = "center", echo = FALSE, out.width = "75%",
#| eval = TRUE
knitr::include_graphics("./figures/Figure_9.png")
```

*Optional: Load the environment after running
`feature_list_workflow(step = 3)`*

```{r load feature_list_workflow_3.RData, echo = TRUE, eval = FALSE, message = FALSE}
# Loads the environment after running `feature_list_workflow(step = 3)
load(
    file = file.path(project_folder, "MZquant_feature_list_workflow_3.RData"),
    envir = .GlobalEnv
)
```

The `feature list workflow` is finalized. The workflow generates several
files in the *feature_list* folder for interest users.

```{r list feature_list folder 2, echo = TRUE, eval = TRUE}
# List files and directories in the feature_list folder
list.files(file.path(project_folder, "results", "feature_lists"),
    include.dirs = TRUE
)
```

## Blank workflow

The blank workflow runs the `blank tagging`, if required. If the blank
correction is not necessary, set blank_correction: FALSE in
MZquant_settings.yaml. For details see also the linked functions.

```{r blank workflow, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# Blank workflow
blank_workflow()
```

`MZquant` integrates two different approaches for blank correction. In
general, a `blank threshold` is calculated of the `blank` values and all
features below this `blank threshold` are deleted of the `feature_list`.

The first option calculates a simple `blank threshold` based on the
`mean` and the `standard deviation`:

$$blankthreshold = mean + blankfactor * sd$$ Where `blankfactor` is a
`numerical` factor to control the fold-change of the
`standard deviation` `sd`.

The second function calculates the `Student t` distribution based
`blank threshold` for each feature based on the
`method detection limit (MDL)` estimation method of US EPA
(@usepa49FR434302011). The method uses a distribution controlled factor
for the addition of the `standard deviation` (`sd`) to the `mean` value:

Case 1: `n >= minimum number of valid blank values`

$$blank threshold = mean + qt(p, df = n - 1) * sd$$

Where `mean` is the `average` of the blank feature values, `qt` is the
`Students t` density function, `p` is the probability, `df` are the
`degrees of freedom`, `n` is the number of blank values and `sd` is the
`standard deviation`.

Case 2: `n < minimum number of valid blank values`

$$blankthreshold = mean + qt(p, df = 1) * sd$$

The parameters `p` (`p` = `alpha`) and `n` (`n` = `blank_qt_threshold`)
are set in the *settings file*.

*Optional: Load the environment after running \`blank_workflow()*

```{r load blank_workflow.RData, echo = TRUE, eval = FALSE, message = FALSE}
# Loads the environment after running `blank_workflow()`
load(
    file = file.path(project_folder, "MZquant_blank_workflow.RData"),
    envir = .GlobalEnv
)
```

## Quantification workflow

The quantification workflow creates the quantification models, enables
trimming of the quantification models and finally performs the
quantification of the targeted compounds.

### Quantification workflow step 1

The `quantification_workflow` step 1 creates the *calibration file
table* `calibration_levels.csv` in ./`results/quantification` for review
and edition of the `calibration levels` and column mapping. The workflow
links the columns with the calibration data with the related
concentration levels.

```{r quantification workflow 1, echo = TRUE, eval = TRUE, message = FALSE}
# quantification workflow step 1
quantification_workflow(steps = 1)
```

The tag for the calibration columns is obtained of
`MZquant_settings.yaml` from section `data:standard`. If the tag
`data:standard_level_pattern` has the correct syntax, the calibration
levels are filled automatically.

**For example:**

**Calibration column name**

`220101_17_ESIpos_Calib_std_500_ngL`

**Standard level pattern**

`data:standard_level_pattern: "Calib_std"`

The calibration level must be separated by the following patterns by an
underline (`_`).

**NOTE**

`MZquant` cannot handle duplicate level annotations in the current
version. In case of several calibration files for one calibration level,
the levels need to be corrected and distinguished by adding a small
decimal to the concentration level, for example `0.001, 0.002, etc.`.

<!-- This is the format for HTML -->

<!-- | filename                            |                   concentration | -->

<!-- |:------------------------------------|--------------------------------:| -->

<!-- | 220101_12_ESIpos_Calib_std_10_ngL   |                              10 | -->

<!-- | 220101_15_ESIpos_Calib_std_100_ngL  |                             100 | -->

<!-- | 220101_18_ESIpos_Calib_std_1000_ngL |                            1000 | -->

<!-- | 220101_xx_ESIpos_Calib_std_10_ngL   |   [10.001]{style="color: red;"} | -->

<!-- | 220101_xx_ESIpos_Calib_std_100_ngL  |  [100.001]{style="color: red;"} | -->

<!-- | 220101_xx_ESIpos_Calib_std_1000_ngL | [1000.001]{style="color: red;"} | -->

<!-- <!-- This is the format for PDF -->

| filename                            |             concentration |
|:------------------------------------|--------------------------:|
| 220101_12_ESIpos_Calib_std_10_ngL   |                        10 |
| 220101_15_ESIpos_Calib_std_100_ngL  |                       100 |
| 220101_18_ESIpos_Calib_std_1000_ngL |                      1000 |
| 220101_xx_ESIpos_Calib_std_10_ngL   |   \textcolor{red}{10.001} |
| 220101_xx_ESIpos_Calib_std_100_ngL  |  \textcolor{red}{100.001} |
| 220101_xx_ESIpos_Calib_std_1000_ngL | \textcolor{red}{1000.001} |

*Optional: Load the environment after running
`quantification_workflow(step = 1)`*

```{r load MZquant_quantification_workflow_1.RData, echo = TRUE, eval = FALSE, message = FALSE}
# Loads the environment after running `quantification_workflow(step = 1)`
load(
    file = file.path(project_folder, "MZquant_quantification_workflow_1.RData"),
    envir = .GlobalEnv
)
```

### Quantification workflow step 2

To load the (edited) `calibration_levels.csv` in the environment run the
`quantification_workflow` step 2. In this step, the
`internal standards (IS)` are assigned to each substance feature and the
relative feature heights (or areas) are calculated by normalization to
the related `IS`. The workflow assigns either the nearest IS (mode
`auto` in the *substance file* column `is_used`) or the predefined IS in
`is_used`.

```{r quantification workflow 2, echo = TRUE, eval = TRUE, message = FALSE}
# quantification workflow
quantification_workflow(steps = 2)
```

The assigned internal standards and the IS calibrated data can be
reviewed in the following files.

```{r list quantification folder 1, echo = TRUE, eval = TRUE}
# List files and directories in the quantification folder
list.files(
    file.path(project_folder, "results", "quantification"),
    pattern = "_IS_", include.dirs = TRUE
)
```

*Optional: Load the environment after running
\`quantification_workflow(step = 2)*

```{r load MZquant_quantification_workflow_2.RData, echo = TRUE, eval = FALSE, message = FALSE}
# Loads the environment after running `quantification_workflow(steps = 2)`
load(
    file = file.path(project_folder, "MZquant_quantification_workflow_2.RData"),
    envir = .GlobalEnv
)
```

*Optional: Review assigned internal standards, reassign internal
standards*

If you are unhappy with the assignments, open the
`substance_Table_IS_assignment_for_review.csv` and add the standardized
name of the internal standard you like to use in the column `IS_used`
and save the file.

Then run the `quantification_workflows(step = 2)` again to apply the
changes to your dataset.

```{r quantification workflow 2 redo, echo = TRUE, eval = FALSE, message = FALSE}
# quantification workflow
quantification_workflow(steps = 2, redo = TRUE)
```

*Optional: Load the environment after running the IS reassignment*

```{r load MZquant_quantification_workflow_redo_2.RData, echo = TRUE, eval = FALSE, message = FALSE}
# Loads the environment after running `quantification_workflow(steps = 2, redo = TRUE)`
load(
    file = file.path(
        project_folder,
        "MZquant_quantification_workflow_redo_2.RData"
    ),
    envir = .GlobalEnv
)
```

### Quantification workflow steps 3-5

The `quantification_workflow` steps 3-5 generates the calibration
models, trimms the models, plots the models, and finally quantifies the
features:

-   step 3: generates generalized additive models (GAM) based on the raw
    data, exports the feature table and plots the draft models.

-   step 4: magic trimming of the quantification models, applying
    automated trimming, and plotting.

-   step 5: applies manual trimming in the trimming table, generates
    trimmed models and plots them, and finally estimates the
    concentrations in unknown samples, quality controls, and
    quantification data and exports the final table.

It is recommended to run the full steps 3-4 to get the results of
automated trimming.

```{r quantification workflow 3, echo = TRUE, eval = TRUE, message = FALSE}
# quantification workflow
quantification_workflow(steps = 3)
```

```{r quantification workflow 4, echo = TRUE, eval = TRUE, message = FALSE}
# quantification workflow
quantification_workflow(steps = 4)
```

*Optional: Load the environments after the last steps* If something went
wrong, try to go back to the previous steps:

```{r load MZquant_quantification_workflow_3.RData, echo = TRUE, eval = FALSE, message = FALSE}
# Loads the environment after running `blank_workflow()`
load(
    file = file.path(project_folder, "MZquant_quantification_workflow_3.RData"),
    envir = .GlobalEnv
)
```

```{r load MZquant_quantification_workflow_4.RData, echo = TRUE, eval = FALSE, message = FALSE}
# Loads the environment after running `blank_workflow()`
load(
    file = file.path(project_folder, "MZquant_quantification_workflow_4.RData"),
    envir = .GlobalEnv
)
```

The purpose of the automated trimming is to limit the calibration models
to a relevant range covering the range of concentrations occurring in
the samples. The idea is to fit the best calibration model possible to
enhance accuracy of the calibration. `MZquant` applies a set of rules to
trim the models (see below). The modeling is based on
`Generalized Additive Models` and includes an automated smoothness
selection (see R package `mgcv` for details).

However, in many cases an automated trimming is not possible and thus
review and manual trimming of compound is required. The following three
files are required for the manual trimming (in folder
`results/quantification`).

```{r list peaklist folder 4, echo = TRUE, eval = TRUE}
# List files and directories in the peaklist folder
list.files(file.path(project_folder, "results", "quantification"),
    pattern = "trimmed", include.dirs = TRUE
)
```

The spreadsheet `quantification_table_trimmed_for_review.csv` is the
main table for reviewing and reprocessing the trimming of the
calibration models. The file
`quantification_table_non-trimmed_for_review.csv` contains the
non-trimmed concentration to relative intensity data. This table is very
useful to restore data, if the original relative intensities have been
manually deleted in the next steps. The pdf-file
`quantification_table_trimmed_model_figs.pdf` includes figs of all
calibration models.

#### Editing the trimming table

-   Open the `quantification_table_trimmed_model_figs.pdf`.

-   Open the `quantification_table_trimmed_for_review.csv` in your
    favorite spreadsheet application.

-   Set the `autofilter` to row 1 ([Figure 10](#fig10)).

-   Insert an empty row above row 1.

-   Hide the first column (`mzquant_id`).

-   Freeze the first column (`stdname`) and the first two rows.

-   Number the calibration columns ("C\_"-columns) beginning from the
    columns with the lowest concentration level up to the highest
    concentration level. The starting number is generally 3 representing
    the position of the lowest concentration level in column C.

-   Select the calibration level columns and the consecutive columns
    `in_sample_min` and `in_sample_max`, add a color gradient and set
    the format to scientific notation for easier review.

```{r}
#| fig10, fig.cap = "Prepare trimming table for review.",
#| fig.align = "center", echo = FALSE, out.width = "100%",
#| eval = TRUE
knitr::include_graphics("./figures/Figure_10.png")
```

It is also recommended to highlight the columns `manual_trim`^[This parameter
controls, if the compound is automatically trimmed (`0`) or not (`1`).
Set it to (`0`) after manual trimming to apply manual trimming. If set to `1`,
trimming is used at all.] and `tracefinder`^[This parameter just is a reminder
evaluate this parameter in a commercial software such as TraceFinder.] with 
`1 = red` and `0 = green` as well as the the good (bad) `gam_r2`
([Figure 11](#fig1)).

```{r}
#| fig11, fig.cap = "Highlight manual\\_trim, tracefinder, and gam\\_r2.",
#| fig.align = "center", echo = FALSE, out.width = "100%",
#| eval = TRUE
knitr::include_graphics("./figures/Figure_11.png")
```

Check now all substances with `0` findings in the samples and set the
manual_trim and the drop_compound columns to `1` ([Figure 12](#fig12)).
The `0` findings could be selected by `NA` (and 0?) in column
`in_sample_num`. In general, the trim and drop columns should be filled
with `1` automatically.

```{r}
#| fig12, fig.cap = "Select manual\\_trim and tracefinder columns.",
#| fig.align = "center", echo = FALSE, out.width = "90%",
#| eval = TRUE
knitr::include_graphics("./figures/Figure_12.png")
```

In the next step, select those rows with only 1 finding in the samples
and lookup for the column `min_sample_name` on the right end of the data
block. Select the first sample after the highest calibration level. This
sample often contains carry-over peaks at a very low level. Compare the
in sample minimal level with the lowest calibration levels and remove
those samples, if the sample level is among the low calibration levels
by setting `manual_trim` to `1` to skip trimming for this compound.

The easy part of the trimming part is done, the more tedious starts now.

*Trimming and rules*

The trimming of the calibration is controlled by the settings in
`min_sample_neighbor_pos` and `max_sample_neighbor_pos` represent the
lower and upper margin of the trimmed calibration curve. Of course, the
limit cannot be below the lowest level and vice versa with the highest
level.

In the best case, the calibration curve is fitted to the next
calibration level below the lowest relative intensity in the samples or
vice verse at the sample maximum.

This automated trimming fails in the following cases (not exhaustive).

Triggered by a rule:

1.  The trimmed range contains an empty field^[Missing values are handled
    as `NA` internally in `MZquant`.] or zero value.

2.  The calibration data contains local minima and/or maxima.

3.  The calibration data is not monotonic increasing.

4.  The minimum sample intensity is below the lower calibration limit.

5.  The maximum sample intensity is above the upper calibration limit.

--\> If you find bugs or cases, report them to Tobias Schulze
([tobias.schulze\@ufz.de](mailto:tobias.schulze@ufz.de){.email}),
please.

*Manual trimming*

In the mentioned cases (and maybe others), manual trimming of the
calibration data is required.

Select all samples again by removing the sample name filter and the
sample number filter. In the `in_sample_num` filter, deselect the `NA` /
`0` rows to hide those rows. In addition deselect those rows which are
`1` in drop for a better overview.

Lookup for the columns `min_sample_neighbor_pos`,
`max_sample_neighbor_pos`, `in_sample_min`, `in_sample_max`,
`calibration_monocity_trim`, `gam_r2`, `localmin_pos` and `localmax_pos`
and decide which rule is valid for the trimming. Check always the
minimal number of calibration points within your calibration margins.

For manual trimming you must be set the lower calibration position or
the upper calibration position according to the sample margins ([Figure
13](#fig13)). Avoid to delete values outside the margins, this is a
possible cause of errors. The will be deleted in the next processing
step automatically.

```{r}
#| fig13,
#| fig.cap = "Example of the trimmed range of the calibration levels.",
#| fig.width = 2, fig.align = "center", echo = FALSE, out.width = "90%",
#| eval = TRUE
knitr::include_graphics("./figures/Figure_13.png")
```

Calibration levels within the upper and lower limit of the trimmed
calibration (`min_sample_neighbor_pos`, `max_sample_neighbor_pos`),
could be removed by deleting the value (for example local minima or
local maxima).

If it is possible to trim the dataset manually, set `manual_trim` to `0`
to apply your settings. If not, you may consider to evaluate in
`TraceFinder` (or another application). Then set the `tracefinder`
column to `1`. You could also comment. The `tracefinder` and
`quantification_comment` is exported in the final output file.

If something went wrong, open the
`MZquant_samples_non-trimmed_for_review.csv` and just copy the
respective `data block` back to the trimmed data file.

It is recommended to save the file as an `ods` or `xlsx` file to keep
the formatting, and to export this spreadsheet as
`MZquant_samples_trimmed_for_review.csv`.

**Important**: Depending on the settings in the spreadsheet application,
the export of the scientific notation might cause loss of decimals and
thus re-format the columns to decimals. In addition, delete the first
column to remove the auxiliary row with the column indexes ([Figure
10](#fig10)) before exporting to the csv file

*An example in the demonstration data*

-   Lookup for the compound `0003_Phenazone`.

-   Set `min_sample_neighbor_pos` = `5`

-   Set `max_sample_neighbor_pos` = `8`

-   Save the `quantification_table_trimmed_for_review.csv` as described
    above.

To override the trimming in demonstration, an edited table can be loaded
in the environment.

```{r read the demonstration trimmed table, echo = TRUE, eval = TRUE, message = FALSE}
# Load the edited  `target_table_for_review.csv`
copy_demo_trimmed_table(project_folder = project_folder)
```

Finally, run the `quantification workflow step 5` to apply the manual
trimming and to quantify.

```{r quantification workflow 5, echo = TRUE, eval = TRUE, message = FALSE}
# quantification workflow
quantification_workflow(steps = 5)
```

The last step can repeated, if you are not happy with the result to try
other manual trimming settings.

\newpage

# Appendix

## Description of the `MZquant_settings.yaml`

The current format was implemented in `MZquant` version 0.8.1. New
settings or changes will be tagged.

### Current settings (0.8.1)

The `MZquant_settings.yaml` is separated in six sections:

-   `settings`: `MZquant_settings.yaml` related parameters

-   `project`: project related parameters

-   `data`: data related parameters

-   `processing`: processing related parameters

-   `quantification`: quantification related parameters

-   `results`: settings to configure final output file with results

### Section `settings`

-   `version`: The version of the default settings file (do not edit).
    The `MZquant` version implementing the current
    `MZquant_settings.yaml` format

### Section `project`

This section includes the general project settings.

-   `data_file`: Name of the data file containing the `MZmine 3` output.

-   `substances_file`: Name of the file containing the
    `substance_table`.

-   `seed`: Integer as seed for random functions.

-   `save_all`: c(`FALSE`, `TRUE`), if `TRUE`, all generated tables are
    exported as `csv` for review or debug (default: `FALSE`).

### Section `data`

This section contains the parameters describing the tagging of the
fields in the `aligned feature list`, exported by `MZmine 3`. A correct
parametrization is a prerequisite to assign the `samples`, `blanks`,
`quantifications`, and `quality controls` to the correct group and to
avoid biased results.

-   `standard`: Enter the term, which identifies the calibrations
    columns (e.g. `Calib`).

-   `standard_level_pattern`: Enter the pattern before the
    quantification level, required for automated assignment of
    quantification levels (e.g. the correct pattern of
    `Calib_water_2018_1000` is `Calib_water_2018`).

-   `quality_control`: Enter the term, defining the `quality control`
    samples (e.g. `QC`).

-   `blank`: Enter the term, which identifies `blanks` (e.g. `Blank`).

-   `calibration_blank`: Enter the term, expressing the specific
    `calibration blank`, which may contain `internal standards` biasing
    the `blank threshold` estimation (e.g. `Calib_std_Blank`).

-   `response`: Enter the response estimation method (c("`area`",
    "`intensity`"), default: `area`.

-   `IS_prefix`: Enter the prefix of the internal standard (e.g.
    "`IS"`).

-   `meta_mzmine`: Metadata fields to be included from `MZmine 3` export
    (default). An updated file will be written to the results folder.
    Including the selected fields and feature information (area or
    height). The values must be quoted, delimeted by comma, and
    emphasized in square brackets, e.g.: ["id", "mz", "mz_range:min",
    "mz_range:max", "rt", "rt_range:min", "rt_range:max",
    compound_db_identity:compound_db_identity",
    compound_db_identity:compound_annotation_score",
    "compound_db_identity:mol_formula",
    "compound_db_identity:precursor_mz",
    "compound_db_identity:mz_diff_ppm", "compound_db_identity:rt",
    "manual_annotation:identity", "manual_annotation:comment",
    "manual_annotation:compound_name"].

-   `meta_substances`: Metadata fields to be included from substance
    file (default).. An updated file will be written to the results
    folder. Including the selected fields and feature information (area
    or height). The values must be quoted, delimited by comma, and
    emphasized in square brackets, e.g.["compound",
    "mzquant:compound_class", "comment", "adduct", "mzquant:mode",
    "mzquant:prim_sec"].

### Section `processing`

This section defines the automated `feature_list_workflow` and
`blank_workflow` processing.

-   `meta_substances_to_feature_list`: c(`TRUE`, `FALSE`), if `TRUE`,
    the meta substances fields are added to revised MZmine 3 table for
    review (default: `TRUE`).

-   `feature_class`: Enter the class which should be processed in blank
    correction c("`tagged`","`all`"), be careful, `all` will last longer
    time due to unprofessional implementation so far.

-   `blank_correction`: c(`TRUE`, `FALSE`), if `FALSE`, the features are
    only tagged with blank thresholds, but not corrected (default:
    `TRUE`)

-   `blank_correction_class`: Enter c("`samples`", "`all`") to select
    the case of `blank_tagging`:

    -   case 1: `samples` - only the features above the
        `blank_threshold` are eliminated in samples.

    -   case 2: `all` - features above the `blank_threshold` are
        eliminated in calibrations and samples.

-   `blank_method`: Method for the blank threshold estimation with
    c("`default`", "`qt`"):

    -   `default`: `mean`(blanks) + `factor` \* `SD`(blanks)

    -   `qt`: `mean`(blanks) + `qt`(`probability`, `degrees of freedom`)
        \* SD(blanks))

-   `blank_qt_alpha`: alpha or probability p for the qt estimation
    (default: `0.99`).

-   `blank_qt_threshold`: Minimum number of valid blank features for the
    use the `qt` estimation (default: `3`).

-   `blank_factor`: Multiplier of the `SD` for the default blank
    correction method (default: `2`).

### Section `quantification`

This section contains the parameters for the `quantification_workflow`.

-   `min_cal_points`: Enter the minimum of calibration points required
    (default: `4`).

-   `in_cal_neighbors`: Enter the minimum of calibration points of the
    minimum sample on the lower end (default: `0`).

-   `max_cal_neighbors`: Enter the minimum of calibration points of the
    maximum sample on the upper end (default: `0`)

-   `low_fig_scale_factor`: Enter the scale factor for fitting the low
    level calibration range. It allows a better visual inspection of the
    low calibration range fits, typical values are `0.1` or `0.05`
    (default: `0.05`).

-   `IS_impute_method`: Enter the method for the `IS gap filling`. Use
    with caution, it is more an experimental method and should be
    mentioned, if used in a productive environment. The default method
    `KNN` imputes missing internal standards by k-nearest neighbor
    imputation. Other methods are considered as not reliable and biased.

-   `colmax`: Minimum of non missing data in columns passed to the
    `impute.KNN::impute` function, expressed in decimals (default:
    `0.8`).

-   `rowmax`: Minimum of non missing data in rows passed to the
    `impute.KNN::impute` function, expressed in decimals (default:
    `0.8`).

-   `IS_method`: Enter the method for the IS assignment for the
    calculation of the concentration ratios with c("`SUBTAB`",
    "`SAMPTAB`") (default: `SUBTAB`)

-   \`SUBTAB : The RTs of the substance_table are used to assign the
    nearest internal standard.

-   `SAMPTAB`: The RTs of the samples table are used to assign the
    nearest internal standard.

-   `n_localminmax`: Number of points which define the range of local
    minima and maxima (default: `2`).

-   `plot_names`: Plot the names of the compounds during modelling
    (helpful for debugging of failing compounds) with c(`FALSE`, `TRUE`)
    (default: `FALSE`)

### Section `results`

This section includes the settings for the final results file.

-   `digits`: Enter number of digits for the output of decimals
    (default: `6`).

-   `unit`: Enter the unit of the final data (default: `ng/L`)

-   `result_metadata`:

-   Selection of metadata columns to be includes in the final output.
    Could be any from `MZmine 3` output, `substance_table`,
    `target_table_for_review`, or `trimmed_for_review`.

-   Optional: The columns can be sorted in your preferred order.

-   **Warning**: The columns related to `MZmine 3` and the
    `substance_table` must be included in the tags `data:meta_mzmine`
    and `data:meta_substances`!

-   The values must be quoted, delimited by comma, and emphasized in
    square brackets (default: ["compound_id", "compound",
    "mzquant:compound_class", "row_mz", "row_rt",
    "compound_db_identity:compound_annotation_score",
    "compound_db_identity:precursor_mz", "row_mz_delta_ppm",
    "manual_annotation:comment", "comment", "adduct", "mzquant:mode",
    "mzquant:prim_sec", "mzquant:class", "gam_r2", "tracefinder",
    "quantification_comment"])

-   **Caution**: Some of the mzmine columns were replaced by names in
    column 1. In these cases, the new name must be entered:

-   `mzquant_id` = "`id`"

-   `row_mz` = "`mz`"

-   `row_mz_min` = "`mz_range:min`"

-   `row_mz_max` = "`mz_range:max`"

-   `row_rt` = "`rt`"

-   `row_rt_min` = "`rt_range:min`"

-   `row_rt_max` = "`rt_range:max`"

-   `row_mz_delta_ppm` = "`compound_db_identity:mz_diff_ppm`"

-   `compound_id` = "`compound_db_identity:compound_db_identity`"

-   `result_classes`: Classes of sample groups quantified and passed to
    the final output with c("`blanks`", "`qc`", "`quantification`",
    "`samples`").

-   The values must be quoted, delimited by comma, and emphasized in
    square brackets (default: ["qc", "quantification", "samples"]).

\newpage

# SessionInfo

```{r sessioninfo, echo = TRUE, eval = TRUE, message = FALSE}
sessionInfo()
```

\newpage

# References

```{r clean_up, echo = FALSE, results = "asis", eval = FALSE}
unlink(project_folder, recursive = TRUE)
```
